# -*- coding: utf-8 -*-
"""M23CSA004.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HvHV9cm9wKaORf-Bg6fi5XDrkAGHy4Yt

**Experiment - 1**

CNN Network using Pytorch
"""

!pip install idx2numpy

import idx2numpy
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

"""importing Pytorch library"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader, TensorDataset
import torchvision.transforms as transforms

"""Check for GPU availability

"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""Load the ubyte file"""

# Load the ubyte file
test_images1 = idx2numpy.convert_from_file('/content/drive/MyDrive/t10k-images.idx3-ubyte')
test_images_labels = idx2numpy.convert_from_file('/content/drive/MyDrive/t10k-labels.idx1-ubyte')
train_images1 = idx2numpy.convert_from_file('/content/drive/MyDrive/train-images.idx3-ubyte')
train_images_labels = idx2numpy.convert_from_file('/content/drive/MyDrive/train-labels.idx1-ubyte')

"""Normalize the training and test dataset"""

train_images = train_images1/255.0
test_images = test_images1/255.0

"""Check shape,dimension,minimum and maximum values of train and test data"""

print("Data Shape:", test_images.shape)
print("Data Type:", test_images.dtype)
print("Min Value:", test_images.min())
print("Max Value:", test_images.max())

print("Data Shape:", test_images_labels.shape)
print("Data Type:", test_images_labels.dtype)
print("Min Value:", test_images_labels.min())
print("Max Value:", test_images_labels.max())

print("Data Shape:", train_images.shape)
print("Data Type:", train_images.dtype)
print("Min Value:", train_images.min())
print("Max Value:", train_images.max())

print("Data Shape:", train_images_labels.shape)
print("Data Type:", train_images_labels.dtype)
print("Min Value:", train_images_labels.min())
print("Max Value:", train_images_labels.max())

"""check the images matrix"""

print(f"Training Example{1}:\n{np.array(train_images[1])}\n")

"""Display some training and test examples"""

# Display the first 10 training examples in a grid layout
plt.figure(figsize=(12, 6))

for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(train_images[i], cmap='gray')
    plt.title(f"Training Example {i}")

plt.tight_layout()
plt.show()

train_images_labels

# Display the first 10 test examples in a grid layout
plt.figure(figsize=(12, 6))

for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(test_images[i], cmap='gray')
    plt.title(f"Testing Example {i}")
    plt.axis('off')

plt.tight_layout()
plt.show()

test_images_labels

# Display the last 10 training examples
start_index = len(train_images) - 10
fig, axs = plt.subplots(2, 5, figsize=(12, 6))

for i in range(start_index, len(train_images)):
    index_within_last_10 = i - start_index
    row = index_within_last_10 // 5
    col = index_within_last_10 % 5

    img = np.array(train_images[i])

    axs[row, col].imshow(img, cmap='gray')
    axs[row, col].set_title(f"Training Example {i}")

plt.tight_layout()
plt.show()

# Display the last 10 test examples
start_index = len(test_images) - 10
fig, axs = plt.subplots(2, 5, figsize=(12, 6))

for i in range(start_index, len(test_images)):
    index_within_last_10 = i - start_index
    row = index_within_last_10 // 5
    col = index_within_last_10 % 5

    img = np.array(test_images[i])

    axs[row, col].imshow(img, cmap='gray')
    axs[row, col].set_title(f"Test Example {i}")

plt.tight_layout()
plt.show()

""" Convert NumPy arrays to PyTorch tensors"""

# Assumingdata is in the shape (num_samples, height, width)
train_images_tensor = torch.FloatTensor(train_images).unsqueeze(1).to(device)  # Add channel dimension
train_labels_tensor = torch.LongTensor(train_images_labels).to(device)
test_images_tensor = torch.FloatTensor(test_images).unsqueeze(1).to(device)  # Add channel dimension
test_labels_tensor = torch.LongTensor(test_images_labels).to(device)

# Create DataLoader for training and testing sets
batch_size = 32
# Training Dataset and DataLoader
train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)

# Testing Dataset and DataLoader
test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

"""CNN Architecture"""

class ConvNeuralNet(nn.Module):
    def __init__(self):
        super(ConvNeuralNet, self).__init__()

        # Convolution Layer 1
        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=7, stride=1, padding=3)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)

        # Convolution Layer 2
        self.conv_layer2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        # Convolution Layer 3
        self.conv_layer3 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=2, padding=1)
        self.avgpool3 = nn.AvgPool2d(kernel_size=2, stride=2)

        # Fully Connected Layer (Output Layer)
        self.fc = nn.Linear(16, 10)  # Adjusted size for the fully connected layer

    def forward(self, x):
        # Forward pass through Convolution Layer 1
        x = self.maxpool1(torch.relu(self.conv_layer1(x)))

        # Forward pass through Convolution Layer 2
        x = self.maxpool2(torch.relu(self.conv_layer2(x)))

        # Forward pass through Convolution Layer 3
        x = self.avgpool3(torch.relu(self.conv_layer3(x)))

        # Reshape for Fully Connected Layer
        x = x.view(-1, 16)

        # Forward pass through Fully Connected Layer (Output Layer)
        x = self.fc(x)

        return x

# Create an instance of the model
model = ConvNeuralNet().to(device)

"""Loss Function and Adam optimizer"""

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

"""Training the model and calculating loss and accuray per epoch"""

# Training
num_epochs = 10
train_losses = []
train_accuracies = []
test_accuracies = []

def calculate_accuracy(loader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            outputs = model(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    accuracy = correct / total
    return accuracy

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        optimizer.zero_grad()

        # Ensure the correct batch size for the output
        outputs = model(inputs)

        loss = criterion(outputs, labels)

        # Ensure the correct batch size for backpropagation
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = correct / total
    train_losses.append(epoch_loss)
    train_accuracies.append(epoch_accuracy)

    print(f'Epoch {epoch + 1}/{num_epochs} => Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')

    # Calculate test accuracy at 1st,6th and 10th epochs
    if epoch + 1 in [1, 6, 10]:
        test_accuracy = calculate_accuracy(test_loader)
        test_accuracies.append(test_accuracy)
        print(f'Test Accuracy at Epoch {epoch + 1}: {test_accuracy:.4f}')

""" Plot accuracy and loss per epoch"""

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Training Accuracy', marker='o')
plt.plot([1, 6, 10], test_accuracies, label='Test Accuracy', marker='o')
plt.title('Training and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""create the confusion matrix"""

# Confusion matrix on the test set
import seaborn as sns
model.eval()
all_labels = []
all_predictions = []

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, predicted = outputs.max(1)
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Confusion matrix
conf_matrix = confusion_matrix(all_labels, all_predictions)
# Plot confusion matrix
color_palette = sns.color_palette("Blues")

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=color_palette, cbar=False, linewidths=0.5, linecolor='gray', square=True)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Total trainable and Non trainable paraeters"""

# Total trainable and non-trainable parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
non_trainable_params = total_params - trainable_params

print(f'Total parameters: {total_params:,}')
print(f'Trainable parameters: {trainable_params:,}')
print(f'Non-trainable parameters: {non_trainable_params:,}')

"""**Experiment -2**

Change in Tensor to get desired Classes in label dataset
"""

# Combine data to create new classes
class_mapping = {
    0: 0, 6: 0,
    1: 1, 7: 1,
    2: 2, 3: 2, 8: 2, 5: 2,
    4: 3, 9: 3
}

# Convert NumPy arrays to PyTorch tensors
train_images_tensor = torch.FloatTensor(train_images).unsqueeze(1)
train_labels_tensor = torch.LongTensor(train_images_labels)

# Modify labels for training set
train_labels_combined = torch.LongTensor([class_mapping[label.item()] for label in train_labels_tensor])

# Create DataLoader for training set
batch_size = 32
train_dataset = TensorDataset(train_images_tensor, train_labels_combined)
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)

# Convert NumPy arrays to PyTorch tensors for testing set
test_images_tensor = torch.FloatTensor(test_images).unsqueeze(1)
test_labels_tensor = torch.LongTensor(test_images_labels)

# Modify labels for testing set
test_labels_combined = torch.LongTensor([class_mapping[label.item()] for label in test_labels_tensor])

# Create DataLoader for testing set
test_dataset = TensorDataset(test_images_tensor, test_labels_combined)
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

"""CNN architecture Same as above"""

# Define the modified CNN architecture
class ModifiedConvNeuralNet(nn.Module):
    def __init__(self, num_classes=4):
        super(ModifiedConvNeuralNet, self).__init__()

        # Convolution Layer 1
        self.conv_layer1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=7, stride=1, padding=3)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2)

        # Convolution Layer 2
        self.conv_layer2 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=5, stride=1, padding=2)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        # Convolution Layer 3
        self.conv_layer3 = nn.Conv2d(in_channels=8, out_channels=4, kernel_size=3, stride=2, padding=1)
        self.avgpool3 = nn.AvgPool2d(kernel_size=2, stride=2)

        # Fully Connected Layer (Output Layer)
        self.fc = nn.Linear(16, 4)

    def forward(self, x):
        # Forward pass through Convolution Layer 1
        x = self.maxpool1(torch.relu(self.conv_layer1(x)))

        # Forward pass through Convolution Layer 2
        x = self.maxpool2(torch.relu(self.conv_layer2(x)))

        # Forward pass through Convolution Layer 3
        x = self.avgpool3(torch.relu(self.conv_layer3(x)))

        # Reshape for Fully Connected Layer
        x = x.view(-1, 16)

        # Forward pass through Fully Connected Layer (Output Layer)
        x = self.fc(x)

        return x

# Create an instance of the modified model
model_modified = ModifiedConvNeuralNet().to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model_modified.parameters())

"""Training and calculating loss and accuracy per epoch"""

# Training
num_epochs = 10
train_losses = []
train_accuracies = []
test_accuracies = []

def calculate_accuracy(loader):
    model_modified.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model_modified(inputs)
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    accuracy = correct / total
    return accuracy

for epoch in range(num_epochs):
    model_modified.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()

        # Ensure the correct batch size for the output
        outputs = model_modified(inputs)

        loss = criterion(outputs, labels)

        # Ensure the correct batch size for backpropagation
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = correct / total
    train_losses.append(epoch_loss)
    train_accuracies.append(epoch_accuracy)

    # Calculate test accuracy at some fixed epochs
    if epoch + 1 in [1, 6, 10]:
        test_accuracy = calculate_accuracy(test_loader)
        test_accuracies.append(test_accuracy)
        print(f'Test Accuracy at Epoch {epoch + 1}: {test_accuracy:.4f}')

    print(f'Epoch {epoch + 1}/{num_epochs} => Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')

"""Plot accuracy and loss per epoch"""

# Training and Test Accuracy
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Training Loss')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(train_accuracies, label='Training Accuracy', marker='o')
plt.plot([1, 6, 10], test_accuracies, label='Test Accuracy', marker='o')
plt.title('Training and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

"""Create Confusion matrix"""

# Confusion matrix on the test set
import seaborn as sns

model.eval()
all_labels = []
all_predictions = []

# Apply class mapping to test labels
mapped_test_labels = [class_mapping[label.item()] for label in all_labels]

# Ensure all classes (0, 1, 2, and 3) are included in the class mapping
class_mapping = {
    0: 0, 6: 0,
    1: 1, 7: 1,
    2: 2, 3: 3, 8: 2, 5: 2,  # Adjusted mapping for class 3
    4: 3, 9: 3
}

with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model_modified(inputs)
        _, predicted = outputs.max(1)
        all_labels.extend(labels.cpu().numpy())
        all_predictions.extend(predicted.cpu().numpy())

# Apply class mapping to test labels
mapped_test_labels = [class_mapping[label] for label in all_labels]

# Compute confusion matrix
conf_matrix = confusion_matrix(mapped_test_labels, all_predictions)

# Define custom color palette
color_palette = sns.color_palette("BuGn")

# Plot confusion matrix with a different style and color palette
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=color_palette, cbar=False, linewidths=0.5, linecolor='gray', square=True)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""Calculate Trainable and Non trainable parameters"""

# Total trainable and non-trainable parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
non_trainable_params = total_params - trainable_params

print(f'Total parameters: {total_params:,}')
print(f'Trainable parameters: {trainable_params:,}')
print(f'Non-trainable parameters: {non_trainable_params:,}')